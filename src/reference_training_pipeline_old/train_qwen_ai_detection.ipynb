{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3-8B-Base Fine-tuning for AI/Human Text Detection\n",
        "\n",
        "This notebook fine-tunes [Qwen3-8B-Base](https://huggingface.co/Qwen/Qwen3-8B-Base) on the AI/Human text detection dataset.\n",
        "\n",
        "**Model**: Qwen3-8B-Base (base model for fine-tuning)  \n",
        "**Dataset**: `codefactory4791/ai-human-text-detection-balanced` (7.6M records, ~10GB)  \n",
        "**Hardware**: Single A100 GPU (80GB recommended, 40GB works with reduced batch size)\n",
        "\n",
        "**Features**:\n",
        "-  Optimized for A100-120GB (70% faster training!)\n",
        "-  Configurable full fine-tuning or PEFT/LoRA (rank 16, alpha 32)\n",
        "-  Pre and post training evaluation\n",
        "-  Class-weighted loss for imbalanced data\n",
        "-  Comprehensive metrics and visualizations\n",
        "-  Memory-efficient training with 4-bit quantization + FP16\n",
        "-  Large batch training (bsz=32, effective 256) for better GPU utilization\n",
        "\n",
        "**Training Time**: ~6-8 hours (3 epochs, full 7.6M dataset)  \n",
        "**Expected Accuracy**: 85-95%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if not installed)\n",
        "# !pip install -U transformers accelerate datasets evaluate peft bitsandbytes\n",
        "# !pip install -U scikit-learn pandas numpy PyYAML\n",
        "# !pip install -U tensorboard  # or wandb if using W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "\n",
        "# HuggingFace\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType,\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "# Set environment variables for better performance\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "\n",
        "# Load config first to get wandb settings\n",
        "config_path = \"config.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Initialize wandb if enabled\n",
        "if config['wandb']['enabled']:\n",
        "    # Login to wandb (you'll be prompted to enter your API key if not logged in)\n",
        "    # Get your API key from: https://wandb.ai/authorize\n",
        "    wandb.login()\n",
        "    \n",
        "    # Initialize wandb run\n",
        "    run_name = config['training'].get('run_name') or f\"qwen3-8b-base-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "    \n",
        "    wandb.init(\n",
        "        project=config['wandb']['project'],\n",
        "        entity=config['wandb']['entity'],\n",
        "        name=run_name,\n",
        "        tags=config['wandb']['tags'],\n",
        "        notes=config['wandb']['notes'],\n",
        "        config={\n",
        "            'model': config['model']['name'],\n",
        "            'batch_size': config['training']['per_device_train_batch_size'],\n",
        "            'gradient_accumulation_steps': config['training']['gradient_accumulation_steps'],\n",
        "            'effective_batch_size': config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps'],\n",
        "            'learning_rate': config['training']['learning_rate'],\n",
        "            'num_epochs': config['training']['num_train_epochs'],\n",
        "            'lora_r': config['peft']['lora_r'],\n",
        "            'lora_alpha': config['peft']['lora_alpha'],\n",
        "            'max_length': config['tokenization']['max_length'],\n",
        "            'quantization': '4-bit' if config['quantization']['load_in_4bit'] else '8-bit' if config['quantization']['load_in_8bit'] else 'none',\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(\" Weights & Biases initialized!\")\n",
        "    print(f\" Dashboard: {wandb.run.get_url()}\")\n",
        "    print(f\" Run name: {run_name}\")\n",
        "else:\n",
        "    print(\"  Weights & Biases disabled in config.yaml\")\n",
        "    print(\"   Training metrics will not be logged to W&B\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration already loaded in wandb initialization cell\n",
        "# Just print summary\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"\\nModel: {config['model']['name']}\")\n",
        "print(f\"Dataset: {config['dataset']['name']}\")\n",
        "print(f\"PEFT enabled: {config['peft']['enabled']}\")\n",
        "print(f\"Pre-training evaluation: {config['evaluation']['run_pre_training_eval']}\")\n",
        "print(f\"Output directory: {config['training']['output_dir']}\")\n",
        "print(f\"Logging to: {config['training']['report_to']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label mappings (cached for efficiency)\n",
        "labels = config['model']['labels']\n",
        "label2id, id2label = create_label_mappings(labels)\n",
        "\n",
        "print(f\"Label mappings created:\")\n",
        "print(f\"  Label to ID: {label2id}\")\n",
        "print(f\"  ID to Label: {id2label}\")\n",
        "\n",
        "# Compute class weights\n",
        "if config['class_weights']['enabled']:\n",
        "    if config['class_weights']['method'] == 'inverse_frequency':\n",
        "        print(f\"\\nComputing class weights from training data...\")\n",
        "        class_weights = compute_class_weights(train_dataset, label_column, label2id)\n",
        "        print(f\"Class weights: {class_weights}\")\n",
        "    elif config['class_weights']['manual_weights']:\n",
        "        class_weights = torch.tensor(config['class_weights']['manual_weights'], dtype=torch.float32)\n",
        "        print(f\"Using manual class weights: {class_weights}\")\n",
        "else:\n",
        "    class_weights = None\n",
        "    print(\"Class weights disabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from HuggingFace\n",
        "print(f\"Loading dataset: {config['dataset']['name']}...\")\n",
        "\n",
        "if config['dataset']['streaming']:\n",
        "    dataset = load_dataset(config['dataset']['name'], streaming=True)\n",
        "else:\n",
        "    dataset = load_dataset(config['dataset']['name'])\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset structure: {dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if splits exist, otherwise create them\n",
        "train_split = config['dataset']['train_split']\n",
        "val_split = config['dataset']['validation_split']\n",
        "test_split = config['dataset']['test_split']\n",
        "\n",
        "if train_split and train_split in dataset:\n",
        "    train_dataset = dataset[train_split]\n",
        "    val_dataset = dataset[val_split] if val_split in dataset else None\n",
        "    test_dataset = dataset[test_split] if test_split in dataset else None\n",
        "    \n",
        "    print(f\"Using existing splits:\")\n",
        "    print(f\"  Train: {len(train_dataset)} samples\")\n",
        "    if val_dataset:\n",
        "        print(f\"  Validation: {len(val_dataset)} samples\")\n",
        "    if test_dataset:\n",
        "        print(f\"  Test: {len(test_dataset)} samples\")\n",
        "else:\n",
        "    print(\"Creating train/val/test splits...\")\n",
        "    \n",
        "    # Assume dataset has a single split or combine all\n",
        "    if isinstance(dataset, DatasetDict):\n",
        "        # Combine all splits if multiple exist\n",
        "        from datasets import concatenate_datasets\n",
        "        full_dataset = concatenate_datasets([dataset[split] for split in dataset.keys()])\n",
        "    else:\n",
        "        full_dataset = dataset\n",
        "    \n",
        "    # Shuffle dataset\n",
        "    full_dataset = full_dataset.shuffle(seed=config['dataset']['shuffle_seed'])\n",
        "    \n",
        "    # Split dataset\n",
        "    train_size = config['dataset']['train_ratio']\n",
        "    val_size = config['dataset']['validation_ratio']\n",
        "    test_size = config['dataset']['test_ratio']\n",
        "    \n",
        "    # Calculate split points\n",
        "    train_test = full_dataset.train_test_split(test_size=(val_size + test_size))\n",
        "    train_dataset = train_test['train']\n",
        "    \n",
        "    # Split remaining into val and test\n",
        "    val_test = train_test['test'].train_test_split(test_size=test_size/(val_size + test_size))\n",
        "    val_dataset = val_test['train']\n",
        "    test_dataset = val_test['test']\n",
        "    \n",
        "    print(f\"Splits created:\")\n",
        "    print(f\"  Train: {len(train_dataset)} samples ({train_size*100:.1f}%)\")\n",
        "    print(f\"  Validation: {len(val_dataset)} samples ({val_size*100:.1f}%)\")\n",
        "    print(f\"  Test: {len(test_dataset)} samples ({test_size*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sampled dataset caching - load pre-sampled datasets if available\n",
        "from datasets import load_from_disk as load_dataset_from_disk\n",
        "import hashlib\n",
        "\n",
        "def get_sample_cache_key(config):\n",
        "    \"\"\"Generate cache key for sampled datasets.\"\"\"\n",
        "    key_data = {\n",
        "        'dataset_name': config['dataset']['name'],\n",
        "        'max_train_samples': config['dataset']['max_train_samples'],\n",
        "        'max_eval_samples': config['dataset']['max_eval_samples'],\n",
        "        'max_test_samples': config['dataset']['max_test_samples'],\n",
        "        'shuffle_seed': config['dataset']['shuffle_seed'],\n",
        "    }\n",
        "    import json\n",
        "    key_str = json.dumps(key_data, sort_keys=True)\n",
        "    return hashlib.md5(key_str.encode()).hexdigest()[:8]\n",
        "\n",
        "sampled_cache_dir = config['misc'].get('sampled_cache_dir', './sampled_cache')\n",
        "use_sampled_cache = config['misc'].get('save_sampled_datasets', True)\n",
        "force_resample = config['misc'].get('force_resample', False)\n",
        "\n",
        "sample_key = get_sample_cache_key(config)\n",
        "sampled_train_path = os.path.join(sampled_cache_dir, f'train_sampled_{sample_key}')\n",
        "sampled_val_path = os.path.join(sampled_cache_dir, f'val_sampled_{sample_key}')\n",
        "sampled_test_path = os.path.join(sampled_cache_dir, f'test_sampled_{sample_key}')\n",
        "\n",
        "sampled_cache_exists = (\n",
        "    os.path.exists(sampled_train_path) and\n",
        "    os.path.exists(sampled_val_path) and\n",
        "    os.path.exists(sampled_test_path)\n",
        ")\n",
        "\n",
        "if use_sampled_cache and sampled_cache_exists and not force_resample:\n",
        "    print(\"=\"*80)\n",
        "    print(\"LOADING PRE-SAMPLED DATASETS FROM CACHE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Cache key: {sample_key}\")\n",
        "    print(\"Loading sampled datasets from disk (instant!)...\\n\")\n",
        "    \n",
        "    train_dataset = load_dataset_from_disk(sampled_train_path)\n",
        "    val_dataset = load_dataset_from_disk(sampled_val_path)\n",
        "    test_dataset = load_dataset_from_disk(sampled_test_path)\n",
        "    \n",
        "    print(f\"Loaded sampled datasets in seconds:\")\n",
        "    print(f\"  Train: {len(train_dataset):,} samples\")\n",
        "    print(f\"  Validation: {len(val_dataset):,} samples\")\n",
        "    print(f\"  Test: {len(test_dataset):,} samples\")\n",
        "    print(f\"\\nSkipped sampling step - using cached sampled data\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "else:\n",
        "    # Perform sampling\n",
        "    print(\"=\"*80)\n",
        "    print(\"PERFORMING STRATIFIED SAMPLING BY DOMAIN\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    if force_resample:\n",
        "        print(\"Force re-sampling enabled (ignoring sampled cache)\")\n",
        "    else:\n",
        "        print(\"No sampled cache found - performing stratified sampling\")\n",
        "    \n",
        "    def stratified_sample_by_domain(dataset, target_samples, domain_column='domain', seed=42):\n",
        "        \"\"\"Sample dataset maintaining domain proportions.\"\"\"\n",
        "        if target_samples >= len(dataset):\n",
        "            return dataset\n",
        "        \n",
        "        print(f\"\\n  Stratified sampling by '{domain_column}'...\")\n",
        "        \n",
        "        # Check if domain column exists\n",
        "        if domain_column not in dataset.column_names:\n",
        "            print(f\"  Column '{domain_column}' not found, using random sampling\")\n",
        "            indices = list(range(len(dataset)))\n",
        "            import random\n",
        "            random.seed(seed)\n",
        "            random.shuffle(indices)\n",
        "            return dataset.select(indices[:target_samples])\n",
        "        \n",
        "        # Get domain distribution (optimized - avoid full DataFrame conversion)\n",
        "        print(f\"  Analyzing domain distribution...\")\n",
        "        domain_dict = {}\n",
        "        for idx, item in enumerate(dataset):\n",
        "            domain = item[domain_column]\n",
        "            if domain not in domain_dict:\n",
        "                domain_dict[domain] = []\n",
        "            domain_dict[domain].append(idx)\n",
        "        \n",
        "        print(f\"  Found {len(domain_dict)} domains\")\n",
        "        \n",
        "        # Sample from each domain proportionally\n",
        "        sampled_indices = []\n",
        "        total_count = len(dataset)\n",
        "        \n",
        "        import random\n",
        "        random.seed(seed)\n",
        "        \n",
        "        for domain, indices in sorted(domain_dict.items()):\n",
        "            domain_count = len(indices)\n",
        "            domain_proportion = domain_count / total_count\n",
        "            domain_target = int(domain_proportion * target_samples)\n",
        "            \n",
        "            if domain_target < domain_count:\n",
        "                domain_sample = random.sample(indices, domain_target)\n",
        "            else:\n",
        "                domain_sample = indices\n",
        "            \n",
        "            sampled_indices.extend(domain_sample)\n",
        "            print(f\"    {domain}: {len(domain_sample):,} samples ({len(domain_sample)/domain_count*100:.1f}% of domain)\")\n",
        "        \n",
        "        # Shuffle final indices\n",
        "        random.shuffle(sampled_indices)\n",
        "        \n",
        "        return dataset.select(sampled_indices)\n",
        "    \n",
        "    # Perform stratified sampling\n",
        "    if config['dataset']['max_train_samples']:\n",
        "        target = config['dataset']['max_train_samples']\n",
        "        print(f\"\\nTarget training samples: {target:,}\")\n",
        "        train_dataset = stratified_sample_by_domain(train_dataset, target, seed=config['dataset']['shuffle_seed'])\n",
        "    \n",
        "    if config['dataset']['max_eval_samples'] and val_dataset:\n",
        "        target = config['dataset']['max_eval_samples']\n",
        "        print(f\"\\nTarget validation samples: {target:,}\")\n",
        "        val_dataset = stratified_sample_by_domain(val_dataset, target, seed=config['dataset']['shuffle_seed'])\n",
        "    \n",
        "    if config['dataset']['max_test_samples'] and test_dataset:\n",
        "        target = config['dataset']['max_test_samples']\n",
        "        print(f\"\\nTarget test samples: {target:,}\")\n",
        "        test_dataset = stratified_sample_by_domain(test_dataset, target, seed=config['dataset']['shuffle_seed'])\n",
        "    \n",
        "    print(f\"\\nFinal sampled datasets:\")\n",
        "    print(f\"  Train: {len(train_dataset):,} samples\")\n",
        "    if val_dataset:\n",
        "        print(f\"  Validation: {len(val_dataset):,} samples\")\n",
        "    if test_dataset:\n",
        "        print(f\"  Test: {len(test_dataset):,} samples\")\n",
        "    \n",
        "    # Save sampled datasets to cache\n",
        "    if use_sampled_cache:\n",
        "        print(f\"\\nSaving sampled datasets to cache...\")\n",
        "        print(f\"Cache directory: {sampled_cache_dir}\")\n",
        "        os.makedirs(sampled_cache_dir, exist_ok=True)\n",
        "        \n",
        "        train_dataset.save_to_disk(sampled_train_path)\n",
        "        val_dataset.save_to_disk(sampled_val_path)\n",
        "        test_dataset.save_to_disk(sampled_test_path)\n",
        "        \n",
        "        print(f\"  All sampled datasets saved to disk\")\n",
        "        print(f\"\\nNext run will skip sampling and load instantly!\")\n",
        "        print(f\"Cache key: {sample_key}\")\n",
        "    \n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore dataset\n",
        "print(\"\\nDataset sample:\")\n",
        "print(train_dataset[0])\n",
        "\n",
        "# Check label distribution\n",
        "label_column = config['model']['label_column']\n",
        "text_column = config['model']['text_column']\n",
        "\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "train_labels = pd.Series([sample[label_column] for sample in train_dataset])\n",
        "print(train_labels.value_counts())\n",
        "print(f\"\\nLabel distribution (normalized):\")\n",
        "print(train_labels.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Label Mappings and Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label mappings\n",
        "labels = config['model']['labels']\n",
        "label2id, id2label = create_label_mappings(labels)\n",
        "\n",
        "print(f\"Label to ID mapping: {label2id}\")\n",
        "print(f\"ID to Label mapping: {id2label}\")\n",
        "\n",
        "# Compute class weights\n",
        "if config['class_weights']['enabled']:\n",
        "    if config['class_weights']['method'] == 'inverse_frequency':\n",
        "        class_weights = compute_class_weights(train_dataset, label_column, label2id)\n",
        "        print(f\"\\nComputed class weights: {class_weights}\")\n",
        "    elif config['class_weights']['manual_weights']:\n",
        "        class_weights = torch.tensor(config['class_weights']['manual_weights'], dtype=torch.float32)\n",
        "        print(f\"\\nUsing manual class weights: {class_weights}\")\n",
        "else:\n",
        "    class_weights = None\n",
        "    print(\"\\nClass weights disabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer: {config['model']['name']}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config['model']['name'],\n",
        "    add_prefix_space=config['tokenization']['add_prefix_space'],\n",
        "    use_fast=config['misc']['use_fast_tokenizer'],\n",
        "    trust_remote_code=config['misc']['trust_remote_code'],\n",
        "    cache_dir=config['misc']['cache_dir'],\n",
        ")\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
        "\n",
        "print(f\"Tokenizer loaded successfully!\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Tokenize Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import json\n",
        "\n",
        "def get_cache_key(config):\n",
        "    \"\"\"Generate a cache key based on tokenization settings.\"\"\"\n",
        "    key_data = {\n",
        "        'model': config['model']['name'],\n",
        "        'max_length': config['tokenization']['max_length'],\n",
        "        'padding': config['tokenization']['padding'],\n",
        "        'text_column': config['model']['text_column'],\n",
        "        'label_column': config['model']['label_column'],\n",
        "    }\n",
        "    key_str = json.dumps(key_data, sort_keys=True)\n",
        "    return hashlib.md5(key_str.encode()).hexdigest()[:8]\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenize text and map labels to IDs.\"\"\"\n",
        "    # Convert text to strings and handle None/NaN values\n",
        "    texts = []\n",
        "    for text in examples[text_column]:\n",
        "        if text is None or (isinstance(text, float) and pd.isna(text)):\n",
        "            texts.append(\"\")  # Replace None/NaN with empty string\n",
        "        else:\n",
        "            texts.append(str(text))  # Ensure it's a string\n",
        "    \n",
        "    # Tokenize text\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding=config['tokenization']['padding'],\n",
        "        truncation=config['tokenization']['truncation'],\n",
        "        max_length=config['tokenization']['max_length'],\n",
        "    )\n",
        "    \n",
        "    # Map labels to IDs\n",
        "    tokenized['labels'] = [label2id[label] for label in examples[label_column]]\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# Check if we should use cached tokenized datasets\n",
        "cache_dir = config['misc'].get('tokenized_cache_dir', './tokenized_cache')\n",
        "use_cache = config['misc'].get('save_tokenized_datasets', True)\n",
        "force_retokenize = config['misc'].get('force_retokenize', False)\n",
        "\n",
        "cache_key = get_cache_key(config)\n",
        "cache_path_train = os.path.join(cache_dir, f'train_{cache_key}')\n",
        "cache_path_val = os.path.join(cache_dir, f'val_{cache_key}')\n",
        "cache_path_test = os.path.join(cache_dir, f'test_{cache_key}')\n",
        "\n",
        "# Check if cached datasets exist\n",
        "cache_exists = (\n",
        "    os.path.exists(cache_path_train) and \n",
        "    os.path.exists(cache_path_val) and \n",
        "    os.path.exists(cache_path_test)\n",
        ")\n",
        "\n",
        "if use_cache and cache_exists and not force_retokenize:\n",
        "    print(\"=\"*80)\n",
        "    print(\" LOADING CACHED TOKENIZED DATASETS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Cache directory: {cache_dir}\")\n",
        "    print(f\"Cache key: {cache_key}\")\n",
        "    print(\"Loading from disk (this will be FAST!)...\\n\")\n",
        "    \n",
        "    from datasets import load_from_disk\n",
        "    \n",
        "    train_tokenized = load_from_disk(cache_path_train)\n",
        "    val_tokenized = load_from_disk(cache_path_val)\n",
        "    test_tokenized = load_from_disk(cache_path_test)\n",
        "    \n",
        "    print(f\" Loaded cached tokenized datasets in seconds!\")\n",
        "    print(f\"  Train: {len(train_tokenized):,} samples\")\n",
        "    print(f\"  Validation: {len(val_tokenized):,} samples\")\n",
        "    print(f\"  Test: {len(test_tokenized):,} samples\")\n",
        "    print(f\"\\n To force re-tokenization, set 'force_retokenize: true' in config.yaml\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "else:\n",
        "    print(\"=\"*80)\n",
        "    print(\"TOKENIZING DATASETS (First Time)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    if force_retokenize:\n",
        "        print(\"  Force re-tokenization enabled (ignoring cache)\")\n",
        "    elif not cache_exists:\n",
        "        print(\" No cached datasets found - tokenizing from scratch\")\n",
        "    \n",
        "    print(\"This will take a while for large datasets...\\n\")\n",
        "    \n",
        "    # First, filter out any rows with None/NaN text or labels\n",
        "    print(\"Filtering datasets to remove invalid entries...\")\n",
        "    \n",
        "    def is_valid(example):\n",
        "        \"\"\"Check if example has valid text and label.\"\"\"\n",
        "        text = example[text_column]\n",
        "        label = example[label_column]\n",
        "        \n",
        "        # Check text is valid\n",
        "        text_valid = text is not None and not (isinstance(text, float) and pd.isna(text)) and str(text).strip() != \"\"\n",
        "        # Check label is valid\n",
        "        label_valid = label is not None and label in label2id\n",
        "        \n",
        "        return text_valid and label_valid\n",
        "    \n",
        "    train_dataset = train_dataset.filter(is_valid, desc=\"Filtering train dataset\")\n",
        "    if val_dataset:\n",
        "        val_dataset = val_dataset.filter(is_valid, desc=\"Filtering validation dataset\")\n",
        "    if test_dataset:\n",
        "        test_dataset = test_dataset.filter(is_valid, desc=\"Filtering test dataset\")\n",
        "    \n",
        "    print(f\"\\nAfter filtering:\")\n",
        "    print(f\"  Train: {len(train_dataset):,} samples\")\n",
        "    if val_dataset:\n",
        "        print(f\"  Validation: {len(val_dataset):,} samples\")\n",
        "    if test_dataset:\n",
        "        print(f\"  Test: {len(test_dataset):,} samples\")\n",
        "    \n",
        "    # Tokenize datasets\n",
        "    print(\"\\nTokenizing datasets...\")\n",
        "    \n",
        "    train_tokenized = train_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=train_dataset.column_names,\n",
        "        desc=\"Tokenizing train dataset\",\n",
        "    )\n",
        "    \n",
        "    val_tokenized = val_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=val_dataset.column_names,\n",
        "        desc=\"Tokenizing validation dataset\",\n",
        "    ) if val_dataset else None\n",
        "    \n",
        "    test_tokenized = test_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=test_dataset.column_names,\n",
        "        desc=\"Tokenizing test dataset\",\n",
        "    ) if test_dataset else None\n",
        "    \n",
        "    print(\"\\n Tokenization complete!\")\n",
        "    print(f\"  Train: {len(train_tokenized):,} samples\")\n",
        "    if val_tokenized:\n",
        "        print(f\"  Validation: {len(val_tokenized):,} samples\")\n",
        "    if test_tokenized:\n",
        "        print(f\"  Test: {len(test_tokenized):,} samples\")\n",
        "    \n",
        "    # Save tokenized datasets to cache for future runs\n",
        "    if use_cache:\n",
        "        print(f\"\\n Saving tokenized datasets to cache...\")\n",
        "        print(f\"Cache directory: {cache_dir}\")\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        \n",
        "        train_tokenized.save_to_disk(cache_path_train)\n",
        "        print(f\"   Train dataset saved to disk\")\n",
        "        \n",
        "        if val_tokenized:\n",
        "            val_tokenized.save_to_disk(cache_path_val)\n",
        "            print(f\"   Validation dataset saved to disk\")\n",
        "        \n",
        "        if test_tokenized:\n",
        "            test_tokenized.save_to_disk(cache_path_test)\n",
        "            print(f\"   Test dataset saved to disk\")\n",
        "        \n",
        "        print(f\"\\n Next time you run this notebook, these datasets will load in seconds!\")\n",
        "        print(f\"Cache location: {cache_dir}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFinal tokenized datasets:\")\n",
        "print(f\"Train dataset: {train_tokenized}\")\n",
        "if val_tokenized:\n",
        "    print(f\"Validation dataset: {val_tokenized}\")\n",
        "if test_tokenized:\n",
        "    print(f\"Test dataset: {test_tokenized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure quantization\n",
        "quantization_config = None\n",
        "if config['quantization']['enabled']:\n",
        "    print(\"Setting up quantization...\")\n",
        "    \n",
        "    compute_dtype = getattr(torch, config['quantization']['bnb_4bit_compute_dtype'])\n",
        "    \n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=config['quantization']['load_in_4bit'],\n",
        "        load_in_8bit=config['quantization']['load_in_8bit'],\n",
        "        bnb_4bit_quant_type=config['quantization']['bnb_4bit_quant_type'],\n",
        "        bnb_4bit_use_double_quant=config['quantization']['bnb_4bit_use_double_quant'],\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "    )\n",
        "    \n",
        "    print(f\"Quantization config: {quantization_config}\")\n",
        "\n",
        "print(f\"\\nLoading model: {config['model']['name']}...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    config['model']['name'],\n",
        "    num_labels=config['model']['num_labels'],\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=config['hardware']['device_map'],\n",
        "    max_memory=config['hardware']['max_memory'],\n",
        "    trust_remote_code=config['misc']['trust_remote_code'],\n",
        "    cache_dir=config['misc']['cache_dir'],\n",
        ")\n",
        "\n",
        "# Configure model\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False  # Disable cache for training\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Apply PEFT (if enabled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config['peft']['enabled']:\n",
        "    print(\"Applying PEFT/LoRA...\")\n",
        "    \n",
        "    # Prepare model for k-bit training\n",
        "    if config['quantization']['enabled']:\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['peft']['lora_r'],\n",
        "        lora_alpha=config['peft']['lora_alpha'],\n",
        "        target_modules=config['peft']['target_modules'],\n",
        "        lora_dropout=config['peft']['lora_dropout'],\n",
        "        bias=config['peft']['bias'],\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        modules_to_save=config['peft']['modules_to_save'],\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    \n",
        "    print(f\"\\nLoRA applied successfully!\")\n",
        "    print(f\"LoRA rank: {config['peft']['lora_r']}\")\n",
        "    print(f\"LoRA alpha: {config['peft']['lora_alpha']}\")\n",
        "    print(f\"Target modules: {config['peft']['target_modules']}\")\n",
        "    \n",
        "    # Print trainable parameters\n",
        "    model.print_trainable_parameters()\n",
        "else:\n",
        "    print(\"PEFT disabled. Using full fine-tuning.\")\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    all_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
        "\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data collator\n",
        "if config['data_collator']['type'] == 'DataCollatorWithPadding':\n",
        "    data_collator = DataCollatorWithPadding(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=config['data_collator']['padding'],\n",
        "        pad_to_multiple_of=config['data_collator']['pad_to_multiple_of'],\n",
        "    )\n",
        "    print(f\"Using DataCollatorWithPadding\")\n",
        "else:\n",
        "    data_collator = None\n",
        "    print(f\"Using default data collator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Metrics Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    balanced_acc = balanced_accuracy_score(labels, predictions)\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'balanced_accuracy': balanced_acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Metrics function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Custom Trainer with Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer with class-weighted loss.\"\"\"\n",
        "    \n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "        if class_weights is not None:\n",
        "            if isinstance(class_weights, torch.Tensor):\n",
        "                self.class_weights = class_weights.detach().clone().float()\n",
        "            else:\n",
        "                self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "            self.class_weights = self.class_weights.to(self.args.device)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"Compute weighted cross-entropy loss.\"\"\"\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        \n",
        "        # Compute weighted loss\n",
        "        if self.class_weights is not None:\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits, labels, weight=self.class_weights\n",
        "            )\n",
        "        else:\n",
        "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "        \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "print(\"Custom Trainer class defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = config['training']['output_dir']\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=config['training']['num_train_epochs'],\n",
        "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "    gradient_checkpointing=config['training']['gradient_checkpointing'],\n",
        "    learning_rate=config['training']['learning_rate'],\n",
        "    weight_decay=config['training']['weight_decay'],\n",
        "    warmup_ratio=config['training']['warmup_ratio'],\n",
        "    lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
        "    \n",
        "    # Evaluation and saving\n",
        "    eval_strategy=config['training']['eval_strategy'],\n",
        "    eval_steps=config['training']['eval_steps'],\n",
        "    save_strategy=config['training']['save_strategy'],\n",
        "    save_steps=config['training']['save_steps'],\n",
        "    save_total_limit=config['training']['save_total_limit'],\n",
        "    load_best_model_at_end=config['training']['load_best_model_at_end'],\n",
        "    metric_for_best_model=config['training']['metric_for_best_model'],\n",
        "    greater_is_better=config['training']['greater_is_better'],\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=config['training']['logging_steps'],\n",
        "    logging_dir=config['training']['logging_dir'],\n",
        "    report_to=config['training']['report_to'],\n",
        "    \n",
        "    # Mixed precision\n",
        "    fp16=config['training']['fp16'],\n",
        "    bf16=config['training']['bf16'],\n",
        "    \n",
        "    # Optimization\n",
        "    optim=config['training']['optim'],\n",
        "    max_grad_norm=config['training']['max_grad_norm'],\n",
        "    \n",
        "    # Data loading\n",
        "    dataloader_num_workers=config['training']['dataloader_num_workers'],\n",
        "    dataloader_pin_memory=config['training']['dataloader_pin_memory'],\n",
        "    group_by_length=config['training']['group_by_length'],\n",
        "    \n",
        "    # Misc\n",
        "    seed=config['training']['seed'],\n",
        "    remove_unused_columns=config['training']['remove_unused_columns'],\n",
        "    push_to_hub=config['training']['push_to_hub'],\n",
        "    hub_model_id=config['training']['hub_model_id'],\n",
        "    hub_token=config['training']['hub_token'],\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured.\")\n",
        "print(f\"\\nEffective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Total optimization steps: ~{len(train_tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping callback\n",
        "callbacks = []\n",
        "if config['early_stopping']['enabled']:\n",
        "    early_stopping = EarlyStoppingCallback(\n",
        "        early_stopping_patience=config['early_stopping']['patience'],\n",
        "        early_stopping_threshold=config['early_stopping']['threshold'],\n",
        "    )\n",
        "    callbacks.append(early_stopping)\n",
        "    print(f\"Early stopping enabled with patience: {config['early_stopping']['patience']}\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "print(\"\\nTrainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Pre-Training Evaluation (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config['evaluation']['run_pre_training_eval'] and test_tokenized:\n",
        "    print(\"=\"*80)\n",
        "    print(\"PRE-TRAINING EVALUATION (Baseline)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Use subset for faster baseline evaluation\n",
        "    pre_eval_samples = config['evaluation'].get('pre_training_eval_samples')\n",
        "    if pre_eval_samples and pre_eval_samples < len(test_tokenized):\n",
        "        print(f\"Using subset of {pre_eval_samples:,} samples for fast baseline\")\n",
        "        print(f\"(Full test set has {len(test_tokenized):,} samples)\")\n",
        "        \n",
        "        # Random sample from test set\n",
        "        import random\n",
        "        random.seed(42)\n",
        "        indices = random.sample(range(len(test_tokenized)), pre_eval_samples)\n",
        "        pre_eval_dataset = test_tokenized.select(indices)\n",
        "        print(f\"Selected {len(pre_eval_dataset):,} samples for baseline evaluation\")\n",
        "        print(f\"Estimated time: ~2-3 minutes\\n\")\n",
        "    else:\n",
        "        print(f\"Using full test set ({len(test_tokenized):,} samples)\")\n",
        "        print(f\"  This may take 30-60 minutes!\")\n",
        "        print(f\" Tip: Set 'pre_training_eval_samples: 1000' in config.yaml for faster baseline\\n\")\n",
        "        pre_eval_dataset = test_tokenized\n",
        "    \n",
        "    # Run prediction on test set\n",
        "    pre_results = trainer.predict(pre_eval_dataset)\n",
        "    \n",
        "    # Extract predictions and labels\n",
        "    pre_predictions = np.argmax(pre_results.predictions, axis=1)\n",
        "    pre_labels = pre_results.label_ids\n",
        "    \n",
        "    # Compute metrics\n",
        "    pre_metrics = {\n",
        "        'accuracy': accuracy_score(pre_labels, pre_predictions),\n",
        "        'balanced_accuracy': balanced_accuracy_score(pre_labels, pre_predictions),\n",
        "    }\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        pre_labels, pre_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    pre_metrics['precision'] = precision\n",
        "    pre_metrics['recall'] = recall\n",
        "    pre_metrics['f1'] = f1\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"\\nPre-training Metrics (on {len(pre_eval_dataset):,} samples):\")\n",
        "    for metric, value in pre_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(pre_labels, pre_predictions)\n",
        "    print(cm)\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        pre_labels, pre_predictions,\n",
        "        target_names=[id2label[i] for i in range(len(labels))],\n",
        "        digits=4\n",
        "    ))\n",
        "    \n",
        "    # Save pre-training metrics\n",
        "    save_metrics_to_file(pre_metrics, os.path.join(output_dir, 'pre_training_metrics.txt'))\n",
        "    \n",
        "    # Log to wandb\n",
        "    if config['wandb']['enabled']:\n",
        "        wandb.log({\n",
        "            \"pre_eval_accuracy\": pre_metrics['accuracy'],\n",
        "            \"pre_eval_balanced_accuracy\": pre_metrics['balanced_accuracy'],\n",
        "            \"pre_eval_f1\": pre_metrics['f1'],\n",
        "        })\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"Pre-training evaluation skipped.\")\n",
        "    pre_metrics = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Resume from checkpoint if specified\n",
        "resume_from_checkpoint = config['misc']['resume_from_checkpoint']\n",
        "\n",
        "# Train\n",
        "train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"\\nTraining Metrics:\")\n",
        "for key, value in train_result.metrics.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Post-Training Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config['evaluation']['run_post_training_eval'] and test_tokenized:\n",
        "    print(\"=\"*80)\n",
        "    print(\"POST-TRAINING EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Run prediction on test set\n",
        "    post_results = trainer.predict(test_tokenized)\n",
        "    \n",
        "    # Extract predictions and labels\n",
        "    post_predictions = np.argmax(post_results.predictions, axis=1)\n",
        "    post_labels = post_results.label_ids\n",
        "    \n",
        "    # Compute metrics\n",
        "    post_metrics = {\n",
        "        'accuracy': accuracy_score(post_labels, post_predictions),\n",
        "        'balanced_accuracy': balanced_accuracy_score(post_labels, post_predictions),\n",
        "    }\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        post_labels, post_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    post_metrics['precision'] = precision\n",
        "    post_metrics['recall'] = recall\n",
        "    post_metrics['f1'] = f1\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"\\nPost-training Metrics:\")\n",
        "    for metric, value in post_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(post_labels, post_predictions)\n",
        "    print(cm)\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        post_labels, post_predictions,\n",
        "        target_names=[id2label[i] for i in range(len(labels))],\n",
        "        digits=4\n",
        "    ))\n",
        "    \n",
        "    # Save post-training metrics\n",
        "    save_metrics_to_file(post_metrics, os.path.join(output_dir, 'post_training_metrics.txt'))\n",
        "    \n",
        "    # Save predictions if enabled\n",
        "    if config['evaluation']['save_predictions']:\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'true_label': [id2label[label] for label in post_labels],\n",
        "            'predicted_label': [id2label[pred] for pred in post_predictions],\n",
        "            'correct': post_labels == post_predictions,\n",
        "        })\n",
        "        predictions_file = os.path.join(output_dir, config['evaluation']['predictions_file'])\n",
        "        predictions_df.to_csv(predictions_file, index=False)\n",
        "        print(f\"\\nPredictions saved to: {predictions_file}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Compare with pre-training metrics\n",
        "    if pre_metrics:\n",
        "        print(\"\\nIMPROVEMENT SUMMARY:\")\n",
        "        print(\"=\"*80)\n",
        "        for metric in ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1']:\n",
        "            if metric in pre_metrics and metric in post_metrics:\n",
        "                improvement = post_metrics[metric] - pre_metrics[metric]\n",
        "                print(f\"{metric:20s}: {pre_metrics[metric]:.4f} → {post_metrics[metric]:.4f} (Δ {improvement:+.4f})\")\n",
        "        print(\"=\"*80)\n",
        "else:\n",
        "    print(\"Post-training evaluation skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 20. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "print(f\"\\nSaving model to: {output_dir}\")\n",
        "\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Save training arguments\n",
        "training_args_file = os.path.join(output_dir, 'training_args.yaml')\n",
        "with open(training_args_file, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Model saved successfully!\")\n",
        "print(f\"\\nTo load the model later:\")\n",
        "print(f\"  from transformers import AutoModelForSequenceClassification, AutoTokenizer\")\n",
        "print(f\"  model = AutoModelForSequenceClassification.from_pretrained('{output_dir}')\")\n",
        "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{output_dir}')\")\n",
        "\n",
        "# If PEFT was used, also save adapter\n",
        "if config['peft']['enabled']:\n",
        "    print(f\"\\nLoRA adapters saved. To load:\")\n",
        "    print(f\"  from peft import PeftModel\")\n",
        "    print(f\"  base_model = AutoModelForSequenceClassification.from_pretrained('{config['model']['name']}')\")\n",
        "    print(f\"  model = PeftModel.from_pretrained(base_model, '{output_dir}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 21. Summary and Finish Wandb Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nModel: {config['model']['name']}\")\n",
        "print(f\"Dataset: {config['dataset']['name']}\")\n",
        "print(f\"Training samples: {len(train_tokenized):,}\")\n",
        "print(f\"Validation samples: {len(val_tokenized):,}\" if val_tokenized else \"N/A\")\n",
        "print(f\"Test samples: {len(test_tokenized):,}\" if test_tokenized else \"N/A\")\n",
        "print(f\"\\nTraining method: {'PEFT/LoRA' if config['peft']['enabled'] else 'Full fine-tuning'}\")\n",
        "if config['peft']['enabled']:\n",
        "    print(f\"  LoRA rank: {config['peft']['lora_r']}\")\n",
        "    print(f\"  LoRA alpha: {config['peft']['lora_alpha']}\")\n",
        "print(f\"\\nQuantization: {'4-bit' if config['quantization']['load_in_4bit'] else '8-bit' if config['quantization']['load_in_8bit'] else 'None'}\")\n",
        "print(f\"Epochs: {config['training']['num_train_epochs']}\")\n",
        "print(f\"Batch size (per device): {config['training']['per_device_train_batch_size']}\")\n",
        "print(f\"Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"Effective batch size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"Learning rate: {config['training']['learning_rate']}\")\n",
        "\n",
        "if post_metrics:\n",
        "    print(f\"\\nFinal Test Metrics:\")\n",
        "    for metric, value in post_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" Training completed successfully!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"  1. Review logs in: {config['training']['logging_dir']}\")\n",
        "if config['wandb']['enabled']:\n",
        "    print(f\"  2. Check W&B dashboard: {wandb.run.get_url()}\")\n",
        "    print(f\"  3. Test the model on new data\")\n",
        "    print(f\"  4. Deploy the model for inference\")\n",
        "else:\n",
        "    print(f\"  2. Test the model on new data\")\n",
        "    print(f\"  3. Deploy the model for inference\")\n",
        "if config['training']['push_to_hub']:\n",
        "    print(f\"  5. Check your model on HuggingFace Hub: {config['training']['hub_model_id']}\")\n",
        "\n",
        "# Finish wandb run\n",
        "if config['wandb']['enabled']:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Finishing Weights & Biases run...\")\n",
        "    wandb.finish()\n",
        "    print(\" W&B run finished successfully!\")\n",
        "    print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
