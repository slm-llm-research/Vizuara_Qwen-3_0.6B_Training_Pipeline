{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3-0.6B Fine-tuning for AI/Human Text Detection\n",
        "\n",
        "This notebook provides a generic training pipeline for fine-tuning Qwen3-0.6B on AI/Human text detection datasets.\n",
        "\n",
        "**Features:**\n",
        "- Dataset selection from 4 available sizes (10k, 100k, 1M, 2M)\n",
        "- Automatic class weight calculation based on label distribution\n",
        "- Configurable hyperparameters\n",
        "- Weights & Biases integration for experiment tracking\n",
        "- Optimized for RunPod A100 GPU\n",
        "- PEFT/LoRA for efficient fine-tuning\n",
        "\n",
        "**Model:** Qwen3-0.6B-Base\n",
        "\n",
        "**Hardware:** Single A100 GPU (40GB/80GB)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# Uncomment the following lines if packages are not installed\n",
        "\n",
        "# !pip install -q -U transformers accelerate datasets evaluate peft bitsandbytes\n",
        "# !pip install -q -U scikit-learn pandas numpy PyYAML\n",
        "# !pip install -q -U wandb tensorboard\n",
        "# !pip install -q -U ipywidgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# HuggingFace\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType,\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Selection\n",
        "\n",
        "Choose the dataset size for training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset mapping\n",
        "DATASET_OPTIONS = {\n",
        "    \"10k\": {\n",
        "        \"name\": \"codefactory4791/raid_aligned_10k\",\n",
        "        \"size\": \"~10K samples\",\n",
        "        \"balance\": \"Balanced (50/50)\",\n",
        "        \"class_weights\": [1.0, 1.0]  # Equal weights for balanced dataset\n",
        "    },\n",
        "    \"100k\": {\n",
        "        \"name\": \"codefactory4791/raid_aligned_100k\",\n",
        "        \"size\": \"~100K samples\",\n",
        "        \"balance\": \"Balanced (50/50)\",\n",
        "        \"class_weights\": [1.0, 1.0]  # Equal weights for balanced dataset\n",
        "    },\n",
        "    \"1M\": {\n",
        "        \"name\": \"codefactory4791/raid_aligned_1000k\",\n",
        "        \"size\": \"~1M samples\",\n",
        "        \"balance\": \"Slightly Imbalanced (AI: 52.17%, Human: 47.83%)\",\n",
        "        \"class_weights\": [0.9167, 1.0909]  # Inverse frequency weights\n",
        "    },\n",
        "    \"2M\": {\n",
        "        \"name\": \"codefactory4791/raid_aligned_2000k\",\n",
        "        \"size\": \"~2M samples\",\n",
        "        \"balance\": \"Imbalanced (AI: 54.57%, Human: 45.43%)\",\n",
        "        \"class_weights\": [0.8324, 1.2009]  # Inverse frequency weights\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display dataset options\n",
        "print(\"Available Datasets:\")\n",
        "print(\"=\" * 80)\n",
        "for key, info in DATASET_OPTIONS.items():\n",
        "    print(f\"\\n{key}:\")\n",
        "    print(f\"  Dataset: {info['name']}\")\n",
        "    print(f\"  Size: {info['size']}\")\n",
        "    print(f\"  Label Balance: {info['balance']}\")\n",
        "    print(f\"  Class Weights (AI_Generated, Human_Written): {info['class_weights']}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SELECT YOUR DATASET HERE\n",
        "# Options: \"10k\", \"100k\", \"1M\", \"2M\"\n",
        "\n",
        "SELECTED_DATASET = \"10k\"  # Change this to your desired dataset\n",
        "\n",
        "# Validate selection\n",
        "if SELECTED_DATASET not in DATASET_OPTIONS:\n",
        "    raise ValueError(f\"Invalid dataset selection. Choose from: {list(DATASET_OPTIONS.keys())}\")\n",
        "\n",
        "# Get dataset info\n",
        "dataset_info = DATASET_OPTIONS[SELECTED_DATASET]\n",
        "DATASET_NAME = dataset_info[\"name\"]\n",
        "CLASS_WEIGHTS = dataset_info[\"class_weights\"]\n",
        "\n",
        "print(f\"Selected Dataset: {SELECTED_DATASET}\")\n",
        "print(f\"HuggingFace Dataset: {DATASET_NAME}\")\n",
        "print(f\"Size: {dataset_info['size']}\")\n",
        "print(f\"Label Balance: {dataset_info['balance']}\")\n",
        "print(f\"Class Weights: {CLASS_WEIGHTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config_path = \"config.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Update config with selected dataset\n",
        "config['dataset']['dataset_name'] = DATASET_NAME\n",
        "config['class_weights']['manual_weights'] = CLASS_WEIGHTS\n",
        "\n",
        "# Update output directory based on dataset\n",
        "config['training']['output_dir'] = f\"./output/qwen3-0.6b-{SELECTED_DATASET}\"\n",
        "config['training']['logging_dir'] = f\"./logs/qwen3-0.6b-{SELECTED_DATASET}\"\n",
        "config['misc']['tokenized_cache_dir'] = f\"./tokenized_cache/{SELECTED_DATASET}\"\n",
        "config['misc']['sampled_cache_dir'] = f\"./sampled_cache/{SELECTED_DATASET}\"\n",
        "\n",
        "# Update run name\n",
        "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "config['training']['run_name'] = f\"qwen3-0.6b-{SELECTED_DATASET}-{timestamp}\"\n",
        "\n",
        "# Update wandb tags\n",
        "config['wandb']['tags'].append(f\"dataset-{SELECTED_DATASET}\")\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"\\nModel: {config['model']['name']}\")\n",
        "print(f\"Dataset: {config['dataset']['dataset_name']}\")\n",
        "print(f\"Output directory: {config['training']['output_dir']}\")\n",
        "print(f\"Run name: {config['training']['run_name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Configuration\n",
        "\n",
        "Modify hyperparameters as needed. Default values are loaded from config.yaml.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display current hyperparameters\n",
        "print(\"Current Hyperparameters:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTraining:\")\n",
        "print(f\"  Epochs: {config['training']['num_train_epochs']}\")\n",
        "print(f\"  Batch Size (per device): {config['training']['per_device_train_batch_size']}\")\n",
        "print(f\"  Gradient Accumulation Steps: {config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective Batch Size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"  Learning Rate: {config['training']['learning_rate']}\")\n",
        "print(f\"  Weight Decay: {config['training']['weight_decay']}\")\n",
        "print(f\"  Warmup Ratio: {config['training']['warmup_ratio']}\")\n",
        "print(f\"  LR Scheduler: {config['training']['lr_scheduler_type']}\")\n",
        "\n",
        "print(f\"\\nLoRA:\")\n",
        "print(f\"  Enabled: {config['peft']['enabled']}\")\n",
        "print(f\"  LoRA Rank: {config['peft']['lora_r']}\")\n",
        "print(f\"  LoRA Alpha: {config['peft']['lora_alpha']}\")\n",
        "print(f\"  LoRA Dropout: {config['peft']['lora_dropout']}\")\n",
        "\n",
        "print(f\"\\nTokenization:\")\n",
        "print(f\"  Max Length: {config['tokenization']['max_length']}\")\n",
        "\n",
        "print(f\"\\nClass Weights:\")\n",
        "print(f\"  Enabled: {config['class_weights']['enabled']}\")\n",
        "print(f\"  Weights (AI_Generated, Human_Written): {config['class_weights']['manual_weights']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nTo modify any hyperparameter, edit the config dictionary in the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Modify hyperparameters here\n",
        "# Uncomment and modify as needed\n",
        "\n",
        "# Example modifications:\n",
        "# config['training']['num_train_epochs'] = 5\n",
        "# config['training']['learning_rate'] = 2e-4\n",
        "# config['training']['per_device_train_batch_size'] = 16\n",
        "# config['peft']['lora_r'] = 32\n",
        "# config['tokenization']['max_length'] = 512\n",
        "\n",
        "print(\"Hyperparameter configuration complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
